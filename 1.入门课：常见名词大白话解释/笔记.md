-- 学习CV第一课（并非完全CV），扫盲一些常见的术语。前提是有一定的工程经验和基本的AI实战经验。为初学者提供一定的学习指导。主要是学习B站课程：飞天闪客UP主的【⚡一口气通关大模型的 100 个关键词【白话DeepSeek07】】https://www.bilibili.com/video/BV1xH5Dz3Eox?vd_source=db337c2a2fa9fcee88212163237d3921 并结合自己的项目经验加以补充的。内容并非完全严谨。



1.函数

​	函数就是有输入，基于一套规则（也可以理解为一个数学公式），算出来一个输出；如y = ax+b这个简单的函数，a和b这些式子中除了输入和输出之外的待定系数叫做参数，我们说的大模型多少b多少b的参数其实就是这种待定系数的个数，如7b的模型，其实是7 billion（70亿个参数）。

​	大模型本质就是一套复杂的数学公式，现在的大语言模型如qwen、gpt4.0底层都是数学公式，你可以理解为y = ax+b就是一个简单的模型。

2.激活函数

​	就是一类经过实践验证的、很适合作为模型的一部分的特殊的函数，是搭建神经网络的关键函数。

3.神经网络【Neural Network】

​	深度学习中，用神经网络表示函数，即神经网络的层和层之间本质就是一个线性函数，外面套一层激活函数。就得到了层和层之间的关系。

4.损失函数【Loss Function】

​	衡量拟合的损失的函数，以曲线拟合的最小二乘法，即线性回归为例，损失函数就是观测值和真实值的差值的平方，最后求和；即残差平方和。

​	模型的训练过程就是调整参数的过程，目的是让损失函数达到最小-前提是不过拟合。

5.反向传播

​	在训练神经网络的过程中，需要不断调整参数，让损失函数的值达到最小，神经网络层和层的本质是一个线性函数外面套一层激活函数，一次不断变换下去，就可以得到整个神经网络（当然我说的是多个X和一个Y的对应关系）；

​	为了让参数为自变量的损失函数降到最低，需要求“偏导数”，实际上采用的是梯度下降的方法，这样损失函数下降的是最快的。而梯度下降就需要用到复合函数求导数，是从外层到内层的，所以是反向传播的。

6.MLP：多层感知机。最经典的神经网络结构。

7.CNN：用于图像处理的卷积神经网络。

8.RNN：用于序列处理的循环神经网络。	

9.Attention机制

​	注意力机制源于一篇论文《Attention is all you need》。注意力机制是现在大模型必不可少的，一会我们会讲解Attention机制的原理。

10.Transformer：基于注意力机制开发的框架。

11.符号主义

​	符号主义指的是人们试图通过显式地寻找一个确定的、可以写出表达式的函数来解释具体问题，如已知两个点，可以确定一条直线。但并非所有的问题都可以用一个显式的函数来表示。我们可能无法找到这个函数。高中的时候物理学过的一些公式，包括类似于“写式子”的操作都是符号主义的思想。

12.联结主义

​	联结主义思想和符号主义是对立的，它直接摆烂了，不寻找一个可以写出表达式的准确反应真实情况的式子，比如给我几个点对，要确定一个函数，这个函数是什么其实我们是不知道的，也可能不是幂函数，可能是非线性的函数，所以我们干脆就不找了，而是用一种特殊的函数操作，根据已有的数据（输入输出的点对的集合），算出来这个特殊函数的参数，就可以近似用这个特殊函数代替（拟合）真实的函数（真实的函数，人无法确定）。

​	特殊函数我们可以自己选择，如机器学习中最简单的任务-线性回归，即用y = ax+b这条直线拟合已有的数据，利用最小二乘法来求出a和b，使得更多的点分居直线两侧。当然这个特殊函数可以更复杂。这种用特殊函数 + 利用已知数据求参数来确定特殊函数的值的思想就是联结主义思想，在AI领域，我们都是基于这种思想。

13.模型【Model】

​	前面提到的特殊函数，如y=ax+b，都属于模型，模型具有输入、输出、参数三个基本部分。模型可以理解为就是一组带参数的数学公式。

14.参数（权重）【Weight】

​	上面提到的数学公式中除了输入输出以外，其它的待定值叫做参数（也叫做权重），只有在有数据的情况下，才可以计算出参数的值，因此，数据决定参数，如你只有两个点之后，你才可以解出来y = ax + b中，a是多少，b是多少； 你只有给我一组点对，我才可以利用曲线拟合最小二乘法来计算出参数a和参数b的值，数据发生变化，基于数据计算出来的模型参数也随着而发生变化。

15.大模型【LM】

​	模型中的参数量特别大，这个模型就叫做大模型，注意大模型不只是局限于我们最常用的豆包这种大语言模型，还有CV领域的图像处理大模型、音视频方面的大模型等等。

16.大语言模型【LLM】：自然语言处理（**NLP**）方面的大模型就叫做大语言模型（Large Language Model - LLM），如豆包底层的大模型，qwen2.5-math-7b，GPT等等。

17.数据集【DataSet】

​	数据集用来训练模型（求参数）或者用来微调，总之是要基于数据集改变模型本身的数据。实际操作中，数据集可能是别人给你提供好的，也可能不提供，需要你自己去找甚至自己构建数据集。

18.训练【Training】

​	模型的训练其实就是根据已有的数据（简单理解为有输入，有对应的输出的这种数据）来代入模型，计算参数的整个过程。尽管参数的计算不像计算最小二乘法a和b那样直接解方程那么简单。需要根据一系列训练代码进行**参数调整**。所以准确来说，训练指的是基于数据集进行参数调整的过程。

19.预训练【Pretraining】

​	**别人**提前基于一定的**海量数据集**把模型训练了一次或者N次，而不是所有参数都是未知数的**模型**。基于裸模型是没办法直接训练好数学模型的，因为它最初不具备语言理解能力，所以必须基于预训练模型。

20.微调【Fine-tuning】

​	基于别人预训练的模型，基于自己的数据集再次训练模型，使得它能够完成特定的任务。如我有一个预训练模型A，只具有基本的理解人的语言的能力，但是数学很差，让它做数学题很垃圾，正确率很低；那如何让它学会数学知识呢，我就需要自己准备数据集，喂给它，然后对它进行微调-你可以简单理解为在特定任务上二次训练，这样它就比之前在数学能力上更牛了。

21.LoRA【Low-Rank Adapt】：一种部分参数微调的方法，低秩微调。 类似还有Adapter、Q-LoRA等。

22.推理【Inference】

​	根据训练好的模型，给它输入，如训练好的LLM，把你想提问的话（输入）喂进去（函数调用），就可以等待它通过模型进行计算，产生输出了，这个计算过程就是推理过程。

​	本质是利用训练好的（参数确定的）模型进行计算的过程。

23.开源模型【Open-source Model】

​	理论上是开放代码 + 训练数据集 + 开放训练好的预训练模型（完全开源模型）（可供别人下载），但实际上目前大部分开源模型只是开放权重（所以可以叫做开放权重模型），即只是你自己可以下载训练好的**模型文件**，里面对你来说还是黑盒子。如DeepSeek。

​	相当于我免费把飞机大炮送给你，你自己加油（推理的成本），你也无法知道飞机是如何造出来的（开源代码 + 训练数据集，最好还给我提供硬件【幻想ing】让我训练）。

​	你可以理解为人家训练好的模型-参数是别人训练好了的，你可以自己部署（下载和运行）在自己的电脑上或者自己的更牛逼的服务器上，自己不需要再基于代码和数据集再训练一次了，而是只需要负责运行模型-即只需要负责进行推理的花费就好。

​	由此可见，训练就是算参数最后得到预训练模型的过程，而推理就是根据预训练模型和输入计算输出的过程，二者都需要大量的计算，大模型的参数越大，训练成本越高，当然，使用起来推理的成本也越高，对设备的要求越高。所以对于一般的开发者而言，开放权重已经足够了，因为自己租不起更买不起那么牛逼的设备来支撑自己训练那么大参数的大模型。所以基于开放权重模型进行大模型的下沉应用开发已经足够了。

24.闭源模型【Closed-source Model】

​	不开放源代码，也不开放权重，只向外提供服务。如ChatGPT、Claude等 -- 只有开放源代码，基于源代码和数据集和硬件设备和时间，才可以训练出来一个预训练模型，即计算出来了模型的权重，才可以对外提供服务。

​	闭源模型只提供服务，就类似于飞机大炮我只是租给你，不是送给你，你只有使用权-需要给我租金，但是我帮你加油（LLM服务的推理也是在云端，即在服务提供商那里，因此你无需自己部署在本地推理）。

25.幻觉【Hallucination  [həˌlus(ə)nˈeɪʃ(ə)n] 】

​	模型胡说的现象。一般指的是在语言上说得通，但是语义上是错误的现象，比如在数学题目推导的过程中，从中间的某一步开始出错，后面基于这种错误都是错误的，这就是LLM出现了幻觉。

26.上下文【Context】

​	对话时，所有输入到LLM中的信息都叫做上下文，包括用户的问题 + 外部知识（可能是联网搜索的知识-开启联网功能之后或者自己构建的私有化知识库-涉及到了RAG技术） + 之前的问答记录；单位是token数。

​	不同的模型有不同的长下文长度限制，一般模型参数越大，支持的上下文长度也越高，但是不绝对，有的LLM专门以较长的上下文为优势。本质上支持的上下文的长度是模型的最大输入token数，这是模型的重要性能指标。

​	单独的LLM如qwen2.5是不具备上下文记忆的能力的，每一次调用都是独立的，即第一次提问和第二次提问本不具有关系，第一次问：我的名字叫做小明，我爱唱民歌，你能给我推荐几首歌么？ 第二次问：我叫什么？ 如果这两次是独立的，那么第二次它是不知道你叫小明的。只有基于LLM，程序员开发上下文记忆的功能（属于下沉应用的范畴），打造一款产品（如豆包）才具有上下文的能力。

27.提示词【Prompt】

​	上下文的别名。因为问LLM也是一门艺术，之前出现过提示词工程师，其实现在的LLM理解自然语言的能力大差不差，所以提问的方式，即表达是否准确和清除就很重要了，这是人的个人能力。从编程的角度来看，所有发给LLM的输入都算提示词：包括用户本来的问题、历史提问、外部知识-Prompt是这些东西的组织形式。从产品的使用者的角度来看，可以简单的把用户的问题当做是Prompt。

28.随机性【Randomness】

​	因为预训练模型是固定的，如果一直取概率最高的那个词作为下一个生成的token，理论上每一次生成的内容都大差不差，所以为了改变模型生成的不确定性，每次让模型生成的都不一样，即改变选取策略（之前是若干个token都会计算得到概率，去概率最大的那个），这就是调整随机性。

​	随机性太高，模型容易胡说；随机性太低，模型回答过于保守。

29.温度【Temperature】

​	温度是控制随机性的指标，一般从0到1，温度越高，随机性越高-模型容易胡说八道；温度越低，回答越保守。在调用LLM的API时，温度一般是必须的入参，和prompt是一样的，都需要传入。

30.Top-K

​	生成当下词之后，要通过模型计算下一个词，控制模型从k个词中选择的过程就叫做Top-K，本质是会对概率进行排序，然后生成一个排行，会在前K个高频token中进行选择，这个过程就叫做Top-K。

31.模型压缩：让模型更小，缩小参数量，以便于个人部署使用。

32.量化：把模型中的参数用更低精度的浮点数表示来降低计算量。

33.蒸馏：用参数量较大的大模型指导参数量较小的小模型的方法。

34.剪枝：删除模型中不必要的神经元来让神经网络更稀疏从而加快速度的方法。

35.思维链：从推理方向来增强模型的能力的方式。

36.私有化部署【Private Deployment】

​	就是把模型文件下载到自己的电脑上（或者自己的服务器上），在自己的电脑上进行推理（这依赖于很强大的硬件，参数越大的模型就越需要牛逼的硬件才可以“带起来”，否则容易小马拉大车-累死了，一般都是利用GPU（显卡）或者CPU，当然你可以租用第三方的硬件设施，越牛逼的越贵），而不需要通过网络，使用第三方提供的服务，如豆包的服务，Deepseek的服务，ChatGPT的服务等。

​	当然你可以自己对这个预训练模型进行二次开发，基于Web，封装成新的API，可以是流式响应的，不管是用Python语言的Web框架还是java的SpringBoot开发Web，都是一样的，这些语言都有库可以读取模型的，给库传入模型在本地的的路径即可。是不是很简单？

37.云【Cloud】、云服务、云服务器、租用服务器

-- 下面的内容，没有计算机网络知识、项目开发经验的人，以及没有部署过项目的人，甚至连项目是什么都没有体会的人是根本看不懂的，所以，多实践，多写点代码，说话才会有味道。

​	云是一个比较庞大的概念，简单说就是第三方厂家，如阿里云，腾讯云，华为云这样的云平台，利用他们提供的云服务，可以在缴纳一定的费用【租金】之后，通过计算机网络，基于人家的硬件设施和服务代码为自己提供方便的功能。往往提供的形式是API，非开发者是很难理解的。

​	为什么需要云呢？这就好比你自己想要体验体验烤肉是什么体验？去吃一次烤肉就可以体验到了，没必要自己买个烤肉的炉子。人家提供服务，你支付一定的费用。云服务器也是一样，云服务器是最大好处是：①首先是服务器，和PC不同，服务器就是部署项目用的，电脑需要一直开着； ②云服务器有稳定的静态IP地址，可以公网访问，自己的电脑只能本地访问，或者和你在同一个局域网下的其它设备访问，部署项目肯定需要公网访问，也只有云服务器可以提供公网IP了。

​	简单说就是云厂家的程序员在人家的某一台服务器上部署了一套时时刻刻在运行的代码，你一旦有了API的调用权利（比如缴纳了足够多的租金，人家会给你秘钥-Key等），就可以自己调用人家的API了，比如代码中传一段音频过去，调用人家的语言转文字API，就可以返回一段转文字之后的结果-一段话。模型是人家的-无需自己本地部署，推理也是人家帮忙完成的-基于人家的服务器完成了推理，人家帮你做了，你只需要支付一定的费用即可-如按照调用次数计费、按照语言的时间长短计费等。

​	云服务器，你是可以通过云厂商的云平台，如阿里云等租用的，你可以租用半年甚至更长时间，你无须担心物理机器坏掉，物理机器由云厂家管理-如阿里巴巴公司，你只需要在自己的电脑上，点开那个网页，通过虚拟机的形式操作那台云服务器，就像你自己的电脑是一样的，在购买之前你可以选择配置，不同的配置的云服务器的租金是不同的，如可以选择内存规模、磁盘规模、CPU规格、是否配置GPU等等，以及选择操作系统，如Ubuntu等Linux或者Windows等，但是租服务器大部分都是为了部署自己的项目，即让前后端代码持续运行，这样就可以源源不断的为外界提供服务了。关于云服务器的更多细节和总结后续再说，就和AI无关了。

38.镜像【Image】

​	如果你本地部署模型，你需要自己配环境，装各种Python的包，很麻烦。但是如果你租用云桌面，人家给你提前配置好环境，如华为云的MindSpore框架，这个打包好的环境就叫做镜像。

​	类似的还有操作系统的镜像，如Ubuntu的镜像，直接下载操作系统镜像到U盘中，就可以开始后续的重装系统的操作啦。

​	还有镜像网站的概念，由于很多正版原版网站都在国外，那么国内想要访问就得开VPN，科学上网才可以，那么国内很多牛人就开发了一款高仿的网站，每天动态更新网站，这样只需要访问这款高仿的网站，就可以享受到国外的资源了。简称为“镜像站”，如github中文网，huggingface中文......

39.下沉应用

​	也可以叫做下层应用、下游应用、下游。本质是对大模型进行包装，根据人们的特殊需求-业务封装成一款产品，可能一个特定的模型，只是这款产品\系统的一个功能模型之一。包含这个LM的系统就可以称之为该模型的下沉应用。

​	总之是基于大模型进行二次开发，不管是根据自己公司的特殊需求进行微调、构造私有知识库进行RAG也好，只是将LM作为API调用，实现模型的包装等等，都算LM的下沉应用。

​	实际上，服创、计设这种比赛就属于做下沉应用的。但是正是这种看上去很高的门槛，让新手误以为门槛很高而不敢接触\或对AI有一定的成见，认为Web和AI的二选一去学习的，其实二者是相辅相成的，做AI的下沉应用的开发也不需要任何数据和AI的知识，有一些流程的了解和基本术语的理解，结合赛题的具体要求，剩下就是结合AI调用开发业务而已，最多自己创新一丢丢就足够拿奖了。本质上也是信息差吧。“多阅读”的含金量还在提高。

40.知识库【KnowledgeBase】

​	知识库是面向RAG而言的，即是外部工具的一部分，为了更好的构建prompt而手动构建的；而知识库是待检索的，根据用户的问题去知识库中寻找关联的知识，一起和用户的问题拼起来辅佐提问的。

41.知识图谱

​	以图数据库存储的图结构的数据库，支持结点的CRUD，关系的CRUD；结点和关系是分开存储的，但是关系是依赖于结点存在的。关系是单向的有向边。

​	常用的图数据库有**Neo4j【基于java】**。经过实践，图数据库如果采用暴力搜索的方式，查询的效率是非常低的。

​	知识图谱常用来存储知识库，方便建立知识之间的关系。

42.RAG【检索增强生成-Retrieval-Augmented Generation】

​	先查资料，然后根据自己构建的私有知识库进行匹配，找到符合需求的知识库，和用户的问题拼起来喂给LLM，和联网搜索的原理是一样的，只是一个从私有知识库搜索，一个从互联网中搜索。

​	核心是构建知识库，然后进行高效的检索。

​	因为有些数据是比较隐私的，是公司内部的一些数据，在互联网上是找不到的，所以就必须搭建私有的知识库，并渴望在喂给LLM之前，先根据用户的问题，到私有化知识库中寻找相关联的知识。

​	检索就是根据用户的问题和知识库中的知识作相似度匹配（前提是先准备好结构化的、私有知识库；然后根据检索策略找到关联的外部知识；然后喂给LLM让LLM生成输出）。

43.联网搜索【Browsing】

​	联网搜索是**为了解决幻觉问题的，其实就是让大模型回答地更好**，依赖于外部知识库，只是这个知识库不是私有化的知识库，而是在互联网上爬取的知识，本质也是别人创作的知识。

44.Token

​	Token我们不翻译，是AI领域的NLP领域中，文本的最小处理单位，一般翻译为“词”，比如我，你，愛，这种都是Token，值得注意的是Token并不一定是一个字母，或者一个汉字，也可能是一个词。很多LLM都是根据输入和输出的token数计费的，比如每一百万Token几分钱这样子。

45.词嵌入【Embedding】

​	把词语-Token映射到高维度的向量的过程叫做Embedding；一个句子映射为一个高维度的向量的步骤大致是：先给句子tokenize化，可以利用jieba这样的分词器进行分词，然后为每一个token作Embedding，然后进行池化等操作来存储每个token在句子中的关系。

46.向量数据库【vector database】

​	要想基于语义进行相似度匹配，两个句子要映射为一个高维的向量，知识库中的每一条知识-一段话，都会被映射为一个高维度的向量，存储在向量数据库中-其实就是一个文件。方便到时候用的时候直接读文件然后检索；而不需要临时给每一个知识库中的知识向量化。相当于VDB是提前给知识库的知识向量化并持久化了。

​	常用的VDB是Faiss，其实就是一个Python的依赖，可以支持文件的向量化，并持久化保存到本地。并在需要的时候读出来，然后调用search方法，传入问题的embedding，就可以得到知识库中每一个向量和问题向量的相似度分数（降序排序完成的）。

47.向量检索【Vector Search】

​	在向量数据库中，利用faiss的search方法，传入问题的Embedding，遍历每一个知识，计算余弦相似度寻找相似度最高的top-k的过程就叫做向量检索。

​	这种检索方法是基于语义的，即字面上他俩很相似，那分数就很高，比如“一元二次方程很简单哦”和“一元三次方程很简单哦”的余弦相似度分数要高于“一元二次方程很困难哦”、“一元二次方程很容易学哦”，当然也可以基于其它的相似度函数，不只是可以计算cos值的。

​	实际上还有BM25这样的基于关键词的检索方法。

​	实际上，不管是语义检索还是关键词检索，都属于线性检索，因为他们的存储方式都是顺序存储，而不是非线性的存储方式，如知识图谱。

​	我们比赛的算法用的是语义检索和关键词检索的加权，结合知识图谱，完成了检索器的“创新”。

48.生成式AI

​	以大语言模型为例，这种给输出，根据大模型一点一点生成文本-注意是一点一点，它是一个词一个词算出来的，后续我们讲Transformer的原理会介绍这个，你就理解为先计算我，然后计算愛，然后计算出你，是基于概率计算的。这种就叫做生成式AI。类似的，在音视频领域也有生成式AI。

49.PGC、UGC、AIGC、AGI

- PGC：Professional Generated Content；由专家创作的内容，如那些技术大拿写的blogger等。
- UGC：User Generated Content；互联网发达之后，人人都是创作者，比如b站的up创作的视频内容。
- AIGC：AI Generated Content；由AI创作或者辅助创作的内容，比如你的大作业就是AIGC哈哈。
- AGI： Artificial General Intelligence 通用人工智能，是一种最终幻想，即集多种功能的模型于一体的总和模型；但是目前话没有实现，目前的模型都专精于一种任务，如语言处理的LLM，视觉图像处理的，语音方面的等等；

50.多模态【MultiModal】

​	现在的产品如豆包都是多模态的，可以处理文本、图片、音频等，集多种输入形式于一体的处理能力叫做多模态。文本属于一种模态，图片输入也是一种模态。其实就是杂混菜。

51.工作流【WorkFlow】

​	将一个特定的任务，通过不同的工具实现，比如先通过LLM生成一段PPT文本内容和框架，然后由PPT生成的网站生成，最后保存到本地。这一套流程都是自动完成的，就需要利用工具设置工作流。

​	常用的工具是扣子【Coze】-傻瓜式。

52.Langchain

​	Langchain是用代码的方式开发工作流，提供了一系列的封装好的工具，你无需在意内部的实现，比如用Langchain可以实现RAG。

​	问题就是可能没有创新，都是调用库。要想自己创新，就得自己手撕RAG框架。

53.智能体【Agent】

​	按照工作流，封装LM和其他涉及到的工具（如软件、Web）进而自动完成复杂任务的系统就是Agent。本质上都是LM的下沉应用。如Manus（昙花一现）等都属于Agent。

​	多个智能体相互协作完成更加复杂的任务叫做多智能体-Multi-Agent。

54.A2A【Agent to Agent Protocol】

​	谷歌公司推出的Agent和Agent之间，以及Agent自己的通信的协议。

55.MCP【Model Context Protocol】

​	MCP是某公司为现在的AI系统操作外部工具和设备推出的一款协议，使得AI系统操作外部工具更规范；

56.常见下层应用产品

* NLP【自然语言处理】：和文本相关。产品有豆包、DeepSeek、Claude、Gemini、GPT、腾讯元宝等；

  * GPT【generative pre-trained transformer】、ChatGPT、OpenAI

    ​	生成式预训练transformer，是一个美国的大模型公司-OpenAI发明的大语言模型。是**ChatGPT**这款聊天**产品**的底层大模型。但是GPT和ChatGPT都是闭源的，即只向外提供服务，所以也被戏称为“CloseAI”。

* CV【计算机视觉】：和图片相关。如绘图相关的闭源产品Midjourney、开源的Stable Diffusion、会话工作流ComfyUI。

* 语音：TTS【文本转语音，Text To Sound】、语音识别【ASR-**Automatic Speech Recognition**】

* 视频：Sora、可灵、即梦等AI视频生成应用；数字人：给一张图片，让他动起来讲课等等其它作用。

* AI编程助手【不是问答助手-支持修改文件的】

  * 以软件形式存在的Cursor：AI编程助手，不只是传统的问答AI，还可以操纵你的文件，支持创建文件等；提高编程效率。
  * 以插件形式存在的Github Copilot。

57.RLHF：人类反馈强化学习。即通过人类的反馈来强化学习让模型说的话更满足人的心意的方式。

58.套壳：封装现有的大模型，对外提供服务。我觉得服创这些比赛，首先要有的就是套壳思维，然后才是“创新”。

59.卖铲子：卖AI辅助课程，DeepSeek教程这种帮助别人学AI的方式，而不直接参与到AI的研发中，在AI的淘金热中帮助别人淘金。

60.CPU、GPU、CUDA、TPU、NPU

* CPU是中央处理单元，但是其并行性并不如显卡GPU计算能力强；在AI领域，大部分都是矩阵运算，可以并行处理，所以大多用GPU进行训练和推理。
* CUDA是英伟达推出的和GPU配套的开发框架。
* TPU是专门用来训练和推理的神经网络计算处理器。
* NPU是专门为终端设备推理的AI加速芯片。

61.vLLM

​	提升大模型的推理速度的推理引擎。

62.Ollama

​	方便开发者本地部署和运行的大模型工具。

63.HuggingFace

​	HuggingFace和Github并列，后者是代码托管和开源平台；前者是模型开源平台，在里面可以找到和下载最新的模型以供自己部署。

64.PyTorch和TensorFlow

​	PyTorch和TensorFlow是Python提供的专门提供AI编程的库，属于深度学习框架。

65.LLama

​	LLaMA（Large Language Model Meta AI）是 Meta AI 发布的一系列基础语言模型，参数规模从 7B 到 65B 不等。不同于其他巨型模型如 GPT-3（175B）和 PaLM（540B），LLaMA 的目标是：在更小模型尺寸下，达到甚至超越主流模型的性能，同时具备开放、可复现的研究价值。

66.Meta

​	Meta原名是FaceBook，是扎克伯格创建的一家美国科技公司。

67.扎克伯格和Linux有什么关系？
	开发Linux的鼻祖是林纳斯，而扎克伯格是开发脸书的，即FaceBook，不是同一个人。



参考资料

B站飞天闪客： https://www.bilibili.com/video/BV1NCgVzoEG9?p=8&vd_source=db337c2a2fa9fcee88212163237d3921



7